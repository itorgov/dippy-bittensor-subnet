# Postmortem Jan 18 2025

# Subnet 11 Model Scoring Incident Postmortem - January 2025

Beginning on Saturday, January 18, 2025, at 17:00 EST, our Dataset API service experienced a significant outage affecting model scoring capabilities across multiple subnets (Subnet 11 and Subnet 58). The incident lasted from January 18 at 17:00 EST until January 21 at 16:00 EST. We were able to restore service progressively, with full restoration completed on January 21.

The incident primarily affected only the Dataset API service. However, the model scoring pipeline relies on fetching data from the Dataset API, thus there were downstream effects on the subnet's capability to process model submissions.
Throughout the incident, other core services continued to function as expected. While there were periods where clients were unable to access the Dataset API, the model scoring pipeline itself remained operational once restored.

This post outlines the events that caused this incident, the architecture we had in place, what failed, what worked and why, and the changes we're making based on what we learned over the last few days.

## Intended Design

Our model scoring pipeline is designed to handle large language model evaluations, including models up to 34B parameters. The system consists of several key components:

1. The Dataset API service - Responsible for serving dataset information to various clients
2. The Validation API - Handles model validation requests
3. Worker evaluation queues - Process the actual model scoring tasks

The Dataset API and Validation API were initially co-located on the same EC2 instance for efficiency. The design allowed for different client groups (subnets 11 and 58) to access the same APIs while maintaining isolation at the application level.
The Worker evaluation queues are hosted separately but rely on the Dataset API for scoring.
The Dataset API is hosted by the subnet team to provide a streaming dataset solution that dataset providers such as Huggingface do not support.

## Initial Incident

On January 17, we launched a new version of our model scoring pipeline with support for 34B parameter models. We anticipated some instability due to the increased resource requirements and communicated this via the discord channel multiple times leading up to the update.

On January 18 at approximately 17:00 EST, the Dataset API service and Validation API service began experiencing crashes due to an exponentially increased traffic volume from two client groups (subnet 11 and subnet 58). Our standard procedure of restarting the EC2 instance proved ineffective as the traffic volume would immediately trigger new crashes.

## Response and Recovery

Our response to the incident proceeded in several phases:

### Phase 1: Initial Triage (January 18)
- Identified that the Dataset API crashes were caused by excessive traffic
- Successfully restored the Validation API service
- Determined that simple restarts were insufficient due to immediate traffic overload

### Phase 2: Service Separation (January 19)
- Split the Dataset API components by subnet
- Prioritized subnet 58 restoration due to its newer components
- Began planning traffic mitigation strategies for subnet 11

### Phase 3: Protected Restoration (January 20)
- Implemented client-specific endpoint restrictions
- Restored Dataset API with enhanced traffic controls
- Validated worker evaluation queue dependencies
- Ran internal pipeline tests to verify functionality

### Phase 4: Full Recovery (January 21)
- Added temporary additional capacity
- Restored worker processes
- Confirmed stable operation as of 16:00 EST

## What Worked Well
- Quick identification of the traffic source issues
- Successful separation of subnet components
- Effective implementation of traffic controls
- Minimal impact to core scoring pipeline functionality

## What Didn't Work Well
- Initial co-location of critical services on a single EC2 instance
- Lack of proactive traffic monitoring and controls
- Insufficient isolation between client groups
- Limited team availability on the weekend

## Remediation Steps

Based on this incident, we are implementing the following changes:

1. Infrastructure Changes
   - Separating critical services onto dedicated instances
   - Implementing automatic scaling based on traffic patterns
   - Adding redundancy for high-traffic components

2. Monitoring and Alerting
   - Enhanced monitoring for Dataset API health
   - New alerts for abnormal traffic patterns
   - Depriving subnet developers of sleep to ensure constant uptime

3. Access Controls
   - Implementation of robust rate limiting
   - Client-specific access controls
   - Traffic pattern analysis tools

4. Process Improvements
   - Development of comprehensive API access documentation
   - Creation of clear escalation procedures
   - Regular capacity planning reviews

## Future Development

We are actively working on several long-term improvements:

1. A new comprehensive solution for public Dataset API access (a beta version is available at https://temp-miner-dataset-sn11.dippy-bittensor-subnet.com)
2. Enhanced subnet isolation mechanisms
3. Automated traffic management systems
4. Improved failover capabilities

While this incident caused disruption during our expected volatile period, it has helped identify critical areas for improvement in our infrastructure. The total downtime was contained to less than 48 hours during business days, and we have already begun implementing measures to prevent similar incidents in the future.
